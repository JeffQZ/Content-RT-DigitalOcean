# 从性能到应用场景，英伟达热门 GPU 对比：H100、A6000、L40S、A100
在人工智能和深度学习领域，GPU 的性能直接影响模型的训练速度和推理效率。随着技术的迅速发展，市场上涌现出多款高性能的 GPU，尤其是英伟达的旗舰产品。本文将对比四款基于 2020 年后架构的显卡：NVIDIA H100、A100、A6000 和 L40S。通过深入分析这些 GPU 的性能指标，我们将探讨它们在模型训练和推理任务中的适用场景，以帮助用户在选择适合的 GPU 时做出明智的决策。同时，我们还会给出一些实际有哪些知名的公司或项目在使用这几款 GPU。

## 主流几款 GPU 中哪些适合推理？哪些适合训练？

那么进行一下指标对比，在 NVIDIA H100、A100、A6000、L40s，这几个GPU 中，分析哪些 GPU更适合 做模型训练任务，哪些 GPU 更适合做推理任务。

以下是 NVIDIA H100、A100、A6000、L40s的主要性能指标参数表：

| **GPU** **型号** | **架构**       | **FP16** **性能** | **FP32性能**  | **显存**      | **显存** **类型** | **带宽**     |
| -------------- | ------------ | --------------- | ----------- | ----------- | ------------- | ---------- |
| **H100**       | Hopper       | 1,671 TFLOPS    | 60 TFLOPS   | 80GB        | HBM3          | 3.9 TB/s   |
| **A100**       | Ampere       | 312 TFLOPS      | 19.5 TFLOPS | 40GB / 80GB | HBM2          | 2,039 GB/s |
| **A6000**      | Ampere       | 77.4 TFLOPS     | 38.7 TFLOPS | 48GB        | GDDR6         | 768 GB/s   |
| **L40s**       | Ada Lovelace | 731 TFLOPS      | 91.6 TFLOPS | 48GB        | GDDR6         | 864GB/s    |

这个表格总结了每个GPU的架构、FP16/FP32计算性能、Tensor Core性能、显存大小、显存类型以及内存带宽，便于比较各个GPU在不同任务场景中的适用性。按照架构来讲，越新的架构肯定性能相对更好，这些架构从旧到新依次是：

1.  **Ampere**（2020年发布）
1.  **Ada** **Lovelace**（2022年发布）
1.  **Hopper**（2022年发布）

在选择用于大语言模型（LLM）训练和推理的GPU时，不同GPU有着各自的特性和适用场景。以下将对这些GPU进行分析，探讨它们在模型训练和推理任务中的优劣势，帮助明确不同GPU的应用场景。

### 1、**NVIDIA H100**

**适用场景**：

-   **模型训练**：H100是目前NVIDIA最先进的GPU，设计专门用于大规模AI训练。它拥有超强的计算能力、超大的显存和极高的带宽，能够处理海量数据，特别适合训练GPT、BERT等大规模语言模型。其Tensor Core性能尤为出色，能够极大加速训练过程。
-   **推理**：H100的性能也能轻松应对推理任务，尤其在处理超大模型时表现优异。但由于其高能耗和成本，一般只在需要极高并发量或实时性要求下用于推理任务。

### **实际用例**

**Inflection** **AI** **：** 在微软和 Nvidia 的支持下，Inflection AI 计划使用[22,000 个 Nvidia H100 计算 GPU](https://www.msn.com/en-gb/money/technology/startup-builds-supercomputer-with-22-000-nvidia-s-h100-compute-gpus/ar-AA1dtt19?ocid=finance-verthp-feeds)（可能与 Frontier 超级计算机的性能相媲美）构建一个超级计算机集群。该集群标志着 Inflection AI 对产品（尤其是其 AI 聊天机器人 Pi）扩展速度和能力的战略投资。

**Meta** **：** 为了支持其开源通用人工智能 (AGI) 计划，[Meta 计划](https://www.techspot.com/news/101585-zuckerberg-meta-set-purchase-350000-nvidia-h100-gpus.html)在 2024 年底前购买 350,000 个 Nvidia H100 GPU。Meta 的大量投资源于其增强先进 AI 功能和可穿戴 AR 技术基础设施的雄心。

### 2、**NVIDIA A100**

**适用场景**：

-   **模型训练**：A100是数据中心AI训练的主力GPU，特别是在混合精度训练中具有极强的表现。其较高的显存和带宽使得它在处理大型模型和大批量训练任务时表现卓越。
-   **推理**：A100的高计算能力和显存也使其非常适合推理任务，特别是在需要处理复杂神经网络和大规模并发请求时表现优异。

### **实际用例**

**Microsoft Azure** **：** Microsoft [Azure 将 A100 GPU 集成](https://azure.microsoft.com/en-us/blog/azure-announces-general-availability-of-scaleup-scaleout-nvidia-a100-gpu-instances-claims-title-of-fastest-public-cloud-super/)到其服务中，以促进公共云中的高性能计算和 AI 可扩展性。这种集成支持各种应用程序，从自然语言处理到复杂的数据分析。

**NVIDIA 的 Selene 超级计算机：** Selene 是一款[NVIDIA DGX SuperPOD 系统，采用 A100 GPU](https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31700/)，在 AI 研究和高性能计算 (HPC) 中发挥了重要作用。值得注意的是，它在科学模拟和 AI 模型的训练时间方面创下了纪录——Selene 在最快工业超级计算机 Top500 榜单中排名第 5。

### 3、**NVIDIA A6000**

**适用场景**：

-   **模型训练**：A6000在工作站环境中是非常合适的选择，特别是在需要大显存的情况下。虽然它的计算能力不如A100或H100，但对于中小型模型的训练已经足够。其显存也能支持较大模型的训练任务。
-   **推理**：A6000的显存和性能使其成为推理的理想选择，尤其是在需要处理较大的输入或高并发推理的场景中，能提供平衡的性能和显存支持。

### **实际应用**

**拉斯维加斯球顶** **巨幕**：[拉斯维加斯的球顶巨幕](https://www.pcmag.com/news/las-vegas-sphere-uses-150-nvidia-a6000-gpus-to-power-its-massive-display)使用了 150 个 NVIDIA A6000 GPU，供其处理和渲染球顶巨幕需要显示的动画内容。


### 4、 **NVIDIA L40s**

**适用场景**：

-   **模型训练**：L40s为工作站设计，并且在计算能力和显存上有较大提升，适合中型到大型模型的训练，尤其是当需要较强的图形处理和AI训练能力结合时。
-   **推理**：L40s的强大性能和大显存使其非常适合高性能推理任务，尤其是在工作站环境下的复杂推理任务。如下图所示，虽然 L40s 的价格比 A100 要低，但是在文生图模型的测试中，它的性能表现比 A100 要高 1.2 倍，这完全是由于其Ada Lovelace Tensor Cores 和 FP8 精度所致。

![](.../pic/l40svsA100.png)

### **实际用例**

**动画工作室：** NVIDIA [L40S 被广泛应用于动画工作室的](https://www.hpcwire.com/2023/10/30/comparing-nvidia-a100-and-nvidia-l40s-which-gpu-is-ideal-for-ai-and-graphics-intensive-workloads/)3D 渲染和复杂视觉效果。其处理高分辨率图形和大量数据的先进功能使其成为媒体和游戏公司制作详细动画和视觉内容的理想选择。

**医疗保健和生命科学：** 医疗保健机构正在利用 L40S 进行基因组分析和医学成像。GPU 在处理大量数据方面的效率正在加速遗传学研究，并通过增强的成像技术提高诊断准确性。

### 结论：

-   **更推荐用于模型训练的** **GPU**：

    -   **H100** 和 **A100** 是目前训练大规模模型（如GPT-3、GPT-4等）的最佳选择，拥有顶级的计算能力、显存和带宽。H100在性能上超越了A100，但A100仍然是当前大规模AI训练中的主力。
    -   **A6000** 可以在工作站环境中进行中小型模型的训练。
    -   **L40S** ：提供均衡的性能，具有出色的 FP32 和 Tensor Core 功能，但在模型训练方面，仍然还是 H100、A100 更强。

<!---->

-   **更推荐用于推理的** **GPU**：

    -   **A6000** 和 **L40s** 是推理任务的理想选择，提供了强大的性能和显存，能够高效处理大模型的推理。
    -   **A100** 和 **H100** 在超大规模并发或实时推理任务中表现优异，但由于其成本相对更高一些，如果只用于推理场景，有些浪费性能，不能物尽其用。

另外，要做大模型的训练必定会需要多张GPU，那么这时候就需要用到 NVIDIA 推出的 NLink 技术。NVLink 通常存在于高端和数据中心级 GPU，但是像 L40s 这样的专业卡不支持 NVLink 的。所以不太适合去做相对复杂的大型模型的训练任务，只建议用单卡训练一些小模型。所以这里更推荐把L40s用于推理任务。

在这里H100是相对最前沿的 GPU 卡型，虽然后来 NVIDIA 发布了 B200，但是这款 GPU 暂时还未大规模在市场上得到应用。像 H100 这种 GPU 实际上既适合做模型训练，也适合做推理，但是 H100 的成本会比较高，性能也比较好，如果只用在推理任务上有些大材小用。

我们以上给出的结论都是基于指标层面，并结合了一些实际用例，大家在选型的过程中还需要结合成本来看。相对于购买 GPU 自己搭建服务器，我们更推荐使用GPU 云服务，一方面它的成本比购买 GPU 更便宜，只需要几分钟就可以开启 GPU 实例，另一方面，个别 GPU 云服务平台还会提供适合团队协作开发的云环境，包括 Jupyter notebook、模型部署等功能。大家可以参考 [DigitalOcean GPU 云服务器](https://www.aidroplet.cn/product/gpu/)定价来看，DigitalOcean 部分型号既提供单卡也提供 8卡的配置，比如 H100 ，而且 [H100 GPU 云服务器正在限时优惠中](https://www.aidroplet.cn/news/3827/)。以下我们可以先参考单卡GPU 实例的价格：

| Name         | GPU (GB) | vCPUs | CPU RAM (GB) | Price (hourly)             | Linux | Windows | Regions | Gradient Subscription Plan |
| ------------ | -------- | ----- | ------------ | -------------------------- | ----- | ------- | ------- | -------------------------- |
| H100         | 80       | 20    | 250          | **限时价$2.5/hr(原价$5.95/hr）** | ✔️    |         | NY2     | N/A                        |
| GPU+ (M4000) | 8        | 8     | 30           | $0.45/hr                   | ✔️    | ✔️      | NY2 CA1 | Free, Pro, and Growth      |
| P4000        | 8        | 8     | 30           | $0.51/hr                   | ✔️    | ✔️      | All     | Free, Pro, and Growth      |
| P5000        | 16       | 8     | 30           | $0.78/hr                   | ✔️    | ✔️      | All     | Pro and Growth             |
| P6000        | 24       | 8     | 30           | $1.10/hr                   | ✔️    | ✔️      | All     | Pro and Growth             |
| RTX4000      | 8        | 8     | 30           | $0.56/hr                   | ✔️    | ✔️      | All     | Free, Pro, and Growth      |
| RTX5000      | 16       | 8     | 30           | $0.82/hr                   | ✔️    | ✔️      | NY2     | Pro and Growth             |
| A4000        | 16       | 8     | 45           | $0.76/hr                   | ✔️    | ✔️      | NY2     | Pro and Growth             |
| A5000        | 24       | 8     | 45           | $1.38/hr                   | ✔️    | ✔️      | NY2     | Growth                     |
| A6000        | 48       | 8     | 45           | $1.89/hr                   | ✔️    | ✔️      | NY2     | Growth                     |
| V100         | 16       | 8     | 30           | $2.30/hr                   | ✔️    |         | NY2 CA1 | Growth                     |
| V100-32G     | 32       | 8     | 30           | $2.30/hr                   | ✔️    |         | NY2     | Growth                     |
| A100         | 40       | 12    | 90           | $3.09/hr                   | ✔️    |         | NY2     | Growth                     |
| A100-80G     | 80       | 12    | 90           | $3.18/hr                   | ✔️    |         | NY2     | Growth                     |

DigitalOcean GPU 云服务是专注 AI 模型训练的云 GPU 服务器租用平台，提供了包括 A5000、A6000、H100 等强大的 GPU 和 IPU 实例，以及透明的定价，可以比其他公共云节省高达70%的计算成本。**如果你感兴趣，希望了解更多，可以加入群聊（QQ群：611945524）直接交流，或访问 aidroplet.cn[联系 DigitalOcean 中国区独家战略合作伙伴卓普云](https://www.aidroplet.cn/contact_sales_gpu/)。**